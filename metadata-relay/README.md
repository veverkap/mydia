# Metadata Relay Service

A caching proxy service for TMDB and TVDB APIs built with Elixir, Plug, and Bandit.

## Overview

The Metadata Relay Service acts as an intermediary between the Mydia application and external metadata providers (TMDB and TVDB). It provides:

- **Caching**: Reduces API calls to external services and improves response times
- **Rate Limiting Protection**: Prevents hitting API rate limits
- **API Key Management**: Centralizes API key handling
- **High Performance**: Built on Bandit HTTP server for excellent throughput

## Technology Stack

- **Elixir**: Functional programming language with OTP supervision
- **Bandit**: Fast, lightweight HTTP/1.1 and HTTP/2 server
- **Plug**: Composable web middleware
- **Req**: Modern HTTP client
- **Cachex**: Powerful in-memory caching with TTL and LRU
- **Jason**: JSON encoding/decoding

## Local Development

### Prerequisites

- Elixir 1.14 or later
- Erlang/OTP 25 or later
- Docker and Docker Compose (alternative to local Elixir install)

### Using Docker (Recommended)

The Docker Compose configuration includes an optional Redis service for persistent caching. By default, Redis is enabled.

**With Redis (default):**

1. **Start all services**:

   ```bash
   docker-compose up -d
   ```

   This starts both the relay service and Redis.

2. **View logs**:

   ```bash
   docker-compose logs -f relay
   ```

3. **Stop all services**:
   ```bash
   docker-compose down
   ```

**Without Redis (in-memory cache only):**

To use in-memory caching instead of Redis:

1. **Edit docker-compose.yml** and comment out:

   - The `REDIS_URL` environment variable in the relay service
   - The entire `redis` service
   - The `depends_on: redis` line
   - The `redis_data` volume

2. **Start the service**:
   ```bash
   docker-compose up -d
   ```

**Accessing Redis directly:**

Redis is exposed on port 6379. You can connect using `redis-cli`:

```bash
docker-compose exec redis redis-cli

# View cache keys
KEYS metadata_relay:*

# View cache stats
INFO stats
```

### Using Local Elixir

1. **Install dependencies**:

   ```bash
   mix deps.get
   ```

2. **Run the server**:

   ```bash
   mix run --no-halt
   ```

3. **Run with iex (interactive shell)**:
   ```bash
   iex -S mix
   ```

### Testing

Run the test suite:

```bash
mix test
```

Run tests with coverage:

```bash
mix test --cover
```

### Code Formatting

Format code according to project standards:

```bash
mix format
```

## Configuration

The service is configured entirely via environment variables for maximum flexibility and security.

### Environment Variables

| Variable       | Required | Default | Description                                                                                                             |
| -------------- | -------- | ------- | ----------------------------------------------------------------------------------------------------------------------- |
| `PORT`         | No       | `4000`  | HTTP port the server listens on                                                                                         |
| `TMDB_API_KEY` | Yes      | -       | API key for The Movie Database. Get one at https://www.themoviedb.org/settings/api                                      |
| `TVDB_API_KEY` | Yes      | -       | API key for TheTVDB. Get one at https://thetvdb.com/api-information                                                     |
| `REDIS_URL`    | No       | -       | Redis connection URL for persistent caching. Format: `redis://[password@]host:port`. If not set, uses in-memory caching |

### Cache Configuration

The metadata relay supports two caching backends:

#### In-Memory Cache (Default)

**When to use:**

- Local development
- Single-instance deployments
- When Redis is not available

**Characteristics:**

- Fast, zero-latency access
- Cache lost on service restart
- Limited by available RAM
- Maximum 20,000 entries with LRU eviction

**Configuration:**

- No configuration needed
- Simply don't set `REDIS_URL`

#### Redis Cache (Optional)

**When to use:**

- Production deployments
- Multi-instance/horizontal scaling
- When cache persistence across restarts is needed
- When sharing cache between multiple services

**Characteristics:**

- Persistent cache across restarts
- Shared cache for multiple instances
- Configurable eviction policies
- Network latency overhead

**Configuration:**
Set the `REDIS_URL` environment variable:

```bash
# Standard Redis
REDIS_URL=redis://localhost:6379

# Redis with password
REDIS_URL=redis://password@localhost:6379

# Redis with username and password
REDIS_URL=redis://username:password@localhost:6379
```

**Fallback behavior:**

- If Redis connection fails at startup, service falls back to in-memory cache
- Service continues to operate normally with degraded caching
- Connection failures are logged but don't crash the service

### Development Configuration

Create a `.env` file in the project root:

**Without Redis (default):**

```bash
PORT=4001
TMDB_API_KEY=your_tmdb_key_here
TVDB_API_KEY=your_tvdb_key_here
```

**With Redis:**

```bash
PORT=4001
TMDB_API_KEY=your_tmdb_key_here
TVDB_API_KEY=your_tvdb_key_here
REDIS_URL=redis://localhost:6379
```

The `.env.example` file provides a template with all available options.

### Cache TTL and Eviction Policies

The service automatically determines cache TTL based on content type:

| Content Type                | TTL     | Rationale                                |
| --------------------------- | ------- | ---------------------------------------- |
| Movie/TV show details by ID | 30 days | Metadata rarely changes once published   |
| Images                      | 90 days | Image URLs never change                  |
| Season/episode data         | 14 days | Episode data is stable once aired        |
| Search results              | 7 days  | Search results change occasionally       |
| Trending content            | 1 hour  | Trending data changes frequently         |
| Default                     | 30 days | Conservative default for other endpoints |

**In-Memory Cache Eviction:**

- Maximum entries: 20,000
- Eviction policy: LRU (Least Recently Used)
- Background cleanup: Every 15 minutes

**Redis Cache Eviction:**

- TTL-based expiration (automatic)
- No size limits (managed by Redis configuration)
- Configurable via Redis `maxmemory-policy` setting

### Production Configuration

In production (Fly.io), environment variables are managed through secrets:

**Without Redis:**

```bash
# Set required secrets
fly secrets set TMDB_API_KEY=your_key_here
fly secrets set TVDB_API_KEY=your_key_here
```

**With Redis:**

```bash
# Set required secrets
fly secrets set TMDB_API_KEY=your_key_here
fly secrets set TVDB_API_KEY=your_key_here

# Add Redis URL (e.g., Upstash, Redis Cloud, or self-hosted)
fly secrets set REDIS_URL=redis://username:password@your-redis-host:6379
```

**Manage secrets:**

```bash
# View configured secrets (values are hidden)
fly secrets list

# Remove a secret
fly secrets unset SECRET_NAME
```

**Security Notes:**

- Never commit API keys to version control
- Use Fly.io secrets for production deployments
- API keys are loaded at runtime via `config/runtime.exs`
- Keys are not logged or exposed in health checks

## Deployment

### Automated Releases (Recommended)

The metadata-relay service uses GitHub Actions for automated CI/CD. When you push a version tag, it automatically:

1. âœ… Runs tests and builds the Docker image
2. ğŸ“¦ Pushes the image to GitHub Container Registry (GHCR)
3. ğŸš€ Deploys to Fly.io production
4. ğŸ“ Creates a GitHub release with notes

#### Creating a Release

To release a new version:

1. **Update the version** in `mix.exs` (if desired):

   ```elixir
   def project do
     [
       version: "0.2.0",  # Bump this version
       # ...
     ]
   end
   ```

2. **Commit your changes**:

   ```bash
   git add .
   git commit -m "feat: prepare metadata-relay v0.2.0 release"
   ```

3. **Create and push a version tag**:

   ```bash
   # Format: metadata-relay-vX.Y.Z
   git tag metadata-relay-v0.2.0
   git push origin metadata-relay-v0.2.0
   ```

4. **Monitor the release**:
   - Go to GitHub Actions to watch the build and deployment
   - The workflow will automatically deploy to Fly.io
   - A GitHub release will be created with deployment details

#### Prerequisites for Automated Releases

**One-time setup** - Add the Fly.io API token to GitHub secrets:

Using the GitHub CLI (recommended):

```bash
# Quick one-liner setup
flyctl auth token | gh secret set FLY_API_TOKEN

# Verify it was set
gh secret list
```

Or manually via the web UI:

1. Get token: `flyctl auth token`
2. Go to: Settings â†’ Secrets and variables â†’ Actions
3. Add secret: Name=`FLY_API_TOKEN`, Value=token from step 1

**That's it!** All releases will now deploy automatically.

**Validate your setup:**

```bash
# Run the validation script to check everything is configured
cd metadata-relay
./scripts/validate-release-setup.sh
```

**Helpful commands:**

```bash
# Monitor a release in real-time
gh run watch

# View recent workflow runs
gh run list --workflow=metadata-relay-release.yml

# Check Fly.io deployment logs
flyctl logs -a metadata-relay -f
```

#### What Gets Built

Each release creates multi-platform Docker images:

- **Platforms**: `linux/amd64`, `linux/arm64`
- **Registry**: GitHub Container Registry (GHCR)
- **Tags**: `latest`, `X.Y.Z`, `X.Y`, `X` (semantic versioning)
- **Access**: `docker pull ghcr.io/yourusername/mydia/metadata-relay:latest`

### Manual Deployment (Fly.io)

For manual deployments or initial setup, you can deploy directly using the Fly CLI.

#### Prerequisites

- Install the Fly CLI: `curl -L https://fly.io/install.sh | sh`
- Sign up and log in: `fly auth login`

#### Initial Deployment

1. **Navigate to the metadata-relay directory**:

   ```bash
   cd metadata-relay
   ```

2. **Launch the app** (first time only):

   ```bash
   fly launch --config fly.toml
   ```

   When prompted:

   - Choose a unique app name (or accept the suggested name)
   - Select a region (default: ewr - Newark, NJ)
   - Skip database creation
   - Skip deployment for now (we need to set secrets first)

3. **Set required secrets**:

   ```bash
   fly secrets set TMDB_API_KEY=your_tmdb_key_here
   fly secrets set TVDB_API_KEY=your_tvdb_key_here
   ```

4. **Deploy the application**:

   ```bash
   fly deploy
   ```

   Database migrations will run automatically on container startup.

5. **Verify deployment**:

   ```bash
   fly open /health
   ```

   This should open your browser to the health check endpoint and show:

   ```json
   {
     "status": "ok",
     "service": "metadata-relay",
     "version": "0.1.0"
   }
   ```

#### Subsequent Deployments

After the initial setup, deploy updates with:

```bash
fly deploy
```

Database migrations will run automatically on container startup.

#### Monitoring

- **View logs**: `fly logs`
- **View real-time logs**: `fly logs -f`
- **Check status**: `fly status`
- **View metrics**: `fly dashboard`

#### Scaling

The default configuration runs 1 machine with 256MB RAM. To scale:

- **Scale vertically** (more resources per machine):

  ```bash
  fly scale vm shared-cpu-2x --memory 512
  ```

- **Scale horizontally** (more machines):
  ```bash
  fly scale count 2
  ```

#### Custom Domain

To use a custom domain:

1. **Add certificate**:

   ```bash
   fly certs add metadata-relay.yourdomain.com
   ```

2. **Configure DNS**: Follow the instructions provided by Fly.io

#### Troubleshooting Deployment

**Check health status:**

```bash
curl https://metadata-relay.fly.dev/health
```

**View application logs:**

```bash
# Real-time logs
fly logs -f

# Last 100 lines
fly logs --limit 100

# Filter by log level
fly logs -f | grep ERROR
```

**SSH into running machine:**

```bash
fly ssh console
```

**Check secrets configuration:**

```bash
fly secrets list
```

**Restart the application:**

```bash
fly apps restart metadata-relay
```

**Check machine status:**

```bash
fly status
fly machines list
```

**Common issues:**

1. **Deployment fails during build:**

   - Check Docker build locally: `docker build -f Dockerfile .`
   - Verify all dependencies in `mix.exs` are available
   - Check build logs: `fly logs`

2. **App crashes after deployment:**

   - Check if secrets are set: `fly secrets list`
   - View crash logs: `fly logs --limit 200`
   - Verify runtime.exs is reading environment variables correctly

3. **Health check failing:**

   - Ensure PORT environment variable matches internal_port in fly.toml
   - Check if application is listening on correct port
   - SSH in and test: `curl localhost:4001/health`

4. **Authentication errors with TMDB/TVDB:**
   - Verify API keys are set correctly: `fly secrets list`
   - Test keys locally first
   - Check for key expiration or quota limits

## API Endpoints

### Health Check

```
GET /health
```

Returns service status and version:

```json
{
  "status": "ok",
  "service": "metadata-relay",
  "version": "0.1.0"
}
```

### TMDB Endpoints

All TMDB endpoints support query parameters compatible with the TMDB API.

- `GET /configuration` - TMDB configuration
- `GET /tmdb/movies/search?query=...` - Search movies
- `GET /tmdb/tv/search?query=...` - Search TV shows
- `GET /tmdb/movies/:id` - Get movie details
- `GET /tmdb/tv/shows/:id` - Get TV show details
- `GET /tmdb/movies/:id/images` - Get movie images
- `GET /tmdb/tv/shows/:id/images` - Get TV show images
- `GET /tmdb/tv/shows/:id/:season` - Get season details
- `GET /tmdb/movies/trending` - Get trending movies
- `GET /tmdb/tv/trending` - Get trending TV shows

### TVDB Endpoints

All TVDB endpoints support query parameters compatible with the TVDB API v4.

- `GET /tvdb/search?query=...` - Search series
- `GET /tvdb/series/:id` - Get series details
- `GET /tvdb/series/:id/extended` - Get extended series details
- `GET /tvdb/series/:id/episodes` - Get series episodes
- `GET /tvdb/seasons/:id` - Get season details
- `GET /tvdb/seasons/:id/extended` - Get extended season details
- `GET /tvdb/episodes/:id` - Get episode details
- `GET /tvdb/episodes/:id/extended` - Get extended episode details
- `GET /tvdb/artwork/:id` - Get artwork details

### Error Tracking Dashboard

The metadata-relay includes an integrated error tracking dashboard powered by ErrorTracker:

```
GET /errors
```

**Features:**

- View all errors and exceptions from the metadata-relay service
- See crash reports submitted by Mydia instances
- Browse error details including stacktraces and context
- Group similar errors together
- Track error frequency and trends

**Access:** Navigate to `https://your-metadata-relay-domain.com/errors` in your browser.

**Note:** Database migrations run automatically on container startup, so the dashboard is available immediately after deployment.

### Crash Report Ingestion

Mydia instances can submit crash reports to the metadata-relay:

```
POST /crashes/report
```

**Request body:**

```json
{
  "error_type": "RuntimeError",
  "error_message": "Something went wrong",
  "stacktrace": [{ "file": "lib/mydia.ex", "line": 42, "function": "process" }],
  "version": "1.0.0",
  "environment": "production",
  "occurred_at": "2025-11-19T23:00:00Z",
  "metadata": {}
}
```

**Rate limiting:** 10 requests per minute per IP address.

## Project Structure

```
metadata-relay/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ metadata_relay/
â”‚   â”‚   â”œâ”€â”€ application.ex     # OTP application supervisor
â”‚   â”‚   â”œâ”€â”€ release.ex         # Release tasks (migrations)
â”‚   â”‚   â””â”€â”€ router.ex          # HTTP router with Plug
â”‚   â”œâ”€â”€ metadata_relay_web/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ layouts/       # Error page templates
â”‚   â”‚   â””â”€â”€ router.ex          # Phoenix router for dashboard
â”‚   â””â”€â”€ metadata_relay.ex      # Main module
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.exs             # Base configuration
â”‚   â”œâ”€â”€ dev.exs                # Development config
â”‚   â”œâ”€â”€ test.exs               # Test config
â”‚   â”œâ”€â”€ prod.exs               # Production config
â”‚   â””â”€â”€ runtime.exs            # Runtime environment config
â”œâ”€â”€ priv/
â”‚   â””â”€â”€ repo/
â”‚       â””â”€â”€ migrations/        # Database migrations
â”œâ”€â”€ test/
â”‚   â””â”€â”€ test_helper.exs        # Test configuration
â”œâ”€â”€ mix.exs                    # Project definition and dependencies
â”œâ”€â”€ Dockerfile                 # Container image definition
â”œâ”€â”€ docker-entrypoint.sh       # Startup script with auto-migrations
â”œâ”€â”€ docker-compose.yml         # Local development setup
â””â”€â”€ README.md                  # This file
```

## Monitoring and Logging

### Error Tracking

The metadata-relay includes integrated error tracking with the ErrorTracker dashboard:

- **Dashboard URL**: `https://your-domain.com/errors`
- **Automatic setup**: Database migrations run automatically on container startup
- **Features**:
  - Track all errors and exceptions from the relay service
  - Receive and view crash reports from Mydia instances
  - Browse error details with full stacktraces
  - Group similar errors together
  - Monitor error frequency and trends

### Application Logging

The service uses Elixir's built-in Logger for structured logging. Logs include:

- **Request/Response Logging**: Automatic via `Plug.Logger`

  - HTTP method, path, status code
  - Response time
  - Client IP address

- **Cache Events**: Logged by `MetadataRelay.Plug.Cache`

  - Cache hits (`:debug` level)
  - Cache misses (`:debug` level)
  - Cache key generation
  - TTL information

- **TVDB Authentication**: Logged by `MetadataRelay.TVDB.Auth`

  - Token generation (`:info` level)
  - Token refresh (`:info` level)
  - Authentication failures (`:error` level)

- **Error Logging**:
  - HTTP errors with status codes and response bodies
  - Network failures with retry attempts
  - Authentication failures with context

### Log Levels

The service uses standard Elixir log levels:

- `:debug` - Detailed information for diagnosing issues (cache events, request details)
- `:info` - General informational messages (startup, authentication events)
- `:warning` - Warning messages (retry attempts, deprecated features)
- `:error` - Error conditions (failed requests, authentication failures)

### Viewing Logs

**Local development:**

```bash
# Logs appear in console when running with mix
mix run --no-halt

# Or in iex
iex -S mix
```

**Docker:**

```bash
docker-compose logs -f relay
```

**Production (Fly.io):**

```bash
# Real-time logs
fly logs -f

# Filter by application name
fly logs -a metadata-relay

# Show errors only
fly logs -f | grep ERROR

# Export logs for analysis
fly logs --limit 1000 > relay-logs.txt
```

### Metrics and Telemetry

The service is instrumented with Elixir's Telemetry library for metrics collection.

**Available Telemetry Events:**

- `[:plug, :router_dispatch, :start]` - Request start
- `[:plug, :router_dispatch, :stop]` - Request complete (includes duration)
- `[:plug, :router_dispatch, :exception]` - Request exception

**Key Metrics to Monitor:**

1. **Request Rate**: Number of requests per second
2. **Response Time**: P50, P95, P99 latencies
3. **Error Rate**: 4xx and 5xx responses
4. **Cache Hit Ratio**: Percentage of requests served from cache
5. **TVDB Token Refresh**: Frequency of token regeneration

### Health Checks

The `/health` endpoint provides basic service status:

```bash
curl https://metadata-relay.fly.dev/health
```

Response:

```json
{
  "status": "ok",
  "service": "metadata-relay",
  "version": "0.1.0"
}
```

A `200 OK` status indicates the service is running and able to respond to requests.

### Performance Monitoring

**Cache Performance:**

- Cache is stored in-memory using ETS
- Default TTL: 1 hour
- No size limit (relies on Fly.io memory constraints)
- Cache is lost on machine restart

**Recommended Monitoring:**

1. Set up Fly.io metrics monitoring for:

   - CPU usage
   - Memory usage
   - Request latency
   - HTTP status codes

2. Configure alerts for:

   - High error rates (>5% 5xx responses)
   - Slow response times (P95 > 1s)
   - Memory usage >80%
   - Machine crashes/restarts

3. Monitor upstream APIs:
   - TMDB API quota and rate limits
   - TVDB API quota and rate limits

## Development Workflow

1. Make changes to source files in `lib/`
2. Run `mix format` to format code
3. Run `mix test` to ensure tests pass
4. Test manually by running the server and making HTTP requests
5. Check logs for any warnings or errors
6. Verify cache behavior for frequently accessed endpoints

## Continuous Integration

The project uses GitHub Actions for automated testing and quality checks:

### CI Workflow (Runs on every push/PR)

Automatically runs on changes to the `metadata-relay/` directory:

- âœ… **Tests**: Full test suite with coverage reporting
- ğŸ” **Code Quality**: Unused dependency checks, compilation warnings check
- ğŸ“ **Formatting**: Ensures code follows project standards
- ğŸ³ **Docker Build**: Verifies Docker image builds successfully

### Release Workflow (Runs on version tags)

Triggered when pushing tags matching `metadata-relay-v*`:

- ğŸ—ï¸ **Build**: Creates multi-platform Docker images (amd64, arm64)
- ğŸ“¦ **Publish**: Pushes to GitHub Container Registry
- ğŸš€ **Deploy**: Automatically deploys to Fly.io production
- ğŸ“ **Release**: Creates GitHub release with deployment notes

All workflows must pass before code can be merged, ensuring production stability.

## Status

- [x] Set up project structure (task 117.1)
- [x] Implement TMDB proxy endpoints (task 117.2)
- [x] Implement TVDB proxy endpoints with authentication (task 117.3)
- [x] Add in-memory caching layer (task 117.4)
- [x] Create production Docker configuration (task 117.5)
- [x] Configure and deploy to Fly.io (task 117.6)
- [x] Update Mydia to use self-hosted relay (task 117.7)
- [x] Add monitoring, logging, and deployment documentation (task 117.8)

**Service URL**: https://metadata-relay.fly.dev

## License

Same as the main Mydia project.
